{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nasty-europe",
   "metadata": {},
   "source": [
    "<h1 align='center'>Python GYM FrozenLake Q-Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-chick",
   "metadata": {},
   "source": [
    "Patryk Kośmider s16863 i Krzysztof Marek s16663"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-classification",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-harvard",
   "metadata": {},
   "source": [
    "https://gym.openai.com/envs/FrozenLake-v0/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-direction",
   "metadata": {},
   "source": [
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend. The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "golden-dancing",
   "metadata": {},
   "source": [
    "![FrozenLake](FrozenLake.png)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "actual-longer",
   "metadata": {},
   "source": [
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "practical-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-manhattan",
   "metadata": {},
   "source": [
    "Stworzenie środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pediatric-flashing",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-format",
   "metadata": {},
   "source": [
    "Tablica przechowywująca dane dla algorytmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mysterious-passport",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-result",
   "metadata": {},
   "source": [
    "Zmnienne ilościowe, gdzie:\n",
    "* **episode_count** - całkowita ilość powtórzeń uczenia się agenta\n",
    "* **step_count** - maksymalna ilość akcji w danym epizodzie\n",
    "* **epoch_count** - co ile epizodów będzie generowane podsumowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "brutal-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_count = 30000\n",
    "step_count = 100\n",
    "epoch_count = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-harvey",
   "metadata": {},
   "source": [
    "Tempo uczenia się agenta, określa w jakim stopniu stara informacja jest nadpisywana przez nową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "derived-glance",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.06"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-increase",
   "metadata": {},
   "source": [
    "Określa jak bardzo agent zwraca uwagę na przyszłe nagrody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sapphire-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-blood",
   "metadata": {},
   "source": [
    "Zmienne ostatecznie określająca jaka akcja zostanie wybrana przez agenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "soviet-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_rate = 1.0\n",
    "min_expl_rate = 0.001\n",
    "max_expl_rate = 0.9\n",
    "expl_decay_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-detector",
   "metadata": {},
   "source": [
    "Trenowanie agenta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-beads",
   "metadata": {},
   "source": [
    "![BellmanEq](BellmanEq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "valued-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards = []\n",
    "\n",
    "for episode in range(episode_count):\n",
    "    state = env.reset()\n",
    "\n",
    "    rewards_sum = 0\n",
    "\n",
    "    for step in range(step_count):\n",
    "        threshold = random.uniform(0, 1)\n",
    "\n",
    "        if threshold > expl_rate:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Równanie Bellmana w formie iteracyjnej\n",
    "        old_value = q_table[state, action]\n",
    "        p = learning_rate\n",
    "        q = 1.0 - learning_rate\n",
    "        best = np.max(q_table[new_state, :])\n",
    "\n",
    "        q_table[state, action] = q * old_value + p * (reward + discount_factor * best)\n",
    "\n",
    "        state = new_state\n",
    "        rewards_sum += reward\n",
    "\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    expl_rate = min_expl_rate + (max_expl_rate - min_expl_rate) * np.exp(-expl_decay_rate * episode)\n",
    "\n",
    "    all_rewards.append(rewards_sum)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-adobe",
   "metadata": {},
   "source": [
    "Wygląd tablicy danych po treningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "authorized-brain",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.54655058 0.46917624 0.4746591  0.46875876]\n [0.23289098 0.22607855 0.21895984 0.4514138 ]\n [0.37036749 0.26425586 0.21491679 0.20674966]\n [0.16333394 0.02619242 0.01173932 0.0139407 ]\n [0.56421131 0.41000003 0.35779265 0.28737961]\n [0.         0.         0.         0.        ]\n [0.19558762 0.17333731 0.22027372 0.10900427]\n [0.         0.         0.         0.        ]\n [0.29600173 0.38791525 0.38714603 0.61018952]\n [0.42440071 0.63352107 0.42042915 0.46552934]\n [0.5462905  0.3755134  0.43306242 0.30098955]\n [0.         0.         0.         0.        ]\n [0.         0.         0.         0.        ]\n [0.42264874 0.52232768 0.73580395 0.55069006]\n [0.71773127 0.86052823 0.7342618  0.7315334 ]\n [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-attack",
   "metadata": {},
   "source": [
    "Wypisanie jakości treningu agenta po konkretnej ilości epizodów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "future-scanning",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1000: 0.036\n2000: 0.184\n3000: 0.445\n4000: 0.612\n5000: 0.67\n6000: 0.697\n7000: 0.725\n8000: 0.724\n9000: 0.731\n10000: 0.71\n11000: 0.692\n12000: 0.716\n13000: 0.709\n14000: 0.723\n15000: 0.724\n16000: 0.74\n17000: 0.715\n18000: 0.733\n19000: 0.74\n20000: 0.726\n21000: 0.728\n22000: 0.753\n23000: 0.721\n24000: 0.735\n25000: 0.716\n26000: 0.743\n27000: 0.712\n28000: 0.737\n29000: 0.709\n30000: 0.72\nAll: 0.66753\n"
     ]
    }
   ],
   "source": [
    "rewards_per_epoch = np.split(np.array(all_rewards), episode_count / epoch_count)\n",
    "\n",
    "for i, r in enumerate(rewards_per_epoch):\n",
    "    print(f'{(i + 1) * epoch_count}: {round(sum(r / epoch_count), 5)}')\n",
    "\n",
    "print(f'All: {round(np.average(np.array(all_rewards)), 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-diversity",
   "metadata": {},
   "source": [
    "Przetestowanie agenta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "brave-circus",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (Down)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\nWygrana\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    for step in range(step_count):\n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "        time.sleep(0.2)\n",
    "        \n",
    "        action = np.argmax(q_table[state, :])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait = True)\n",
    "            \n",
    "            env.render()\n",
    "            \n",
    "            if reward == 1:\n",
    "                print('Wygrana')\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print('Przegrana')\n",
    "                time.sleep(3)\n",
    "            \n",
    "            clear_output(wait = True)\n",
    "            break\n",
    "        \n",
    "        state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-multimedia",
   "metadata": {},
   "source": [
    "Zamknięcie środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "statutory-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}